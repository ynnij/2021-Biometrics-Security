{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1871068_김혜진_멀티모달_2차_코드_모델2개.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c5cfd094ed634cbf839d05275b174248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_953d4da48ba842e6aa76a7af89c22919",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a03b7da19ddd4e51a7f9bdd5fe0474fd",
              "IPY_MODEL_e2105ac4e3e542ad83de99b92e4142d1"
            ]
          }
        },
        "953d4da48ba842e6aa76a7af89c22919": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a03b7da19ddd4e51a7f9bdd5fe0474fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e35b259cdd034f89b2496c70e307ee49",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2333242d83c445ba9b74ff4d6cec3449"
          }
        },
        "e2105ac4e3e542ad83de99b92e4142d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_646d41ea185e400090f2ba858854944d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [1:11:53&lt;00:00, 10.9kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0147ac3cac104166bd2b7133d1aee70d"
          }
        },
        "e35b259cdd034f89b2496c70e307ee49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2333242d83c445ba9b74ff4d6cec3449": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "646d41ea185e400090f2ba858854944d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0147ac3cac104166bd2b7133d1aee70d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmhHFu3RDHmM",
        "outputId": "386526f4-80c0-4858-b193-dde868ec5c13"
      },
      "source": [
        "from google.colab import drive  # 드라이브 마운트\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2uI8FXgDn4M",
        "outputId": "f5ce6734-427d-430f-9cd3-542dd5b64b1f"
      },
      "source": [
        "# 필요 라이브러리 import \n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "import time\n",
        "import copy\n",
        "%matplotlib inline  \n",
        "%config InlineBackend.figure_format='retina'\n",
        "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
        "\n",
        "# Device Configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print (\"This notebook use [%s].\"%(device))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version:[1.8.1+cu101].\n",
            "This notebook use [cuda:0].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vupOAMcDq0A"
      },
      "source": [
        "EPOCHS =100\n",
        "BATCH_SIZE= 32"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLrXwDuwEC7x"
      },
      "source": [
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "\n",
        "transforms = transforms.Compose([transforms.ToTensor(),\n",
        "                                 transforms.Resize((224,224)),\n",
        "                                 transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                                                      [0.229, 0.224, 0.225])])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "DATASET_PATH = \"/content/drive/MyDrive/생체인증보안/multi\"\n",
        "\n",
        "#face dataset 불러오기\n",
        "train_Fdata = datasets.ImageFolder(DATASET_PATH + '/train/face', transform=transforms)\n",
        "val_Fdata = datasets.ImageFolder(DATASET_PATH + '/validate/face', transform=transforms)\n",
        "\n",
        "train_Fiter = torch.utils.data.DataLoader(train_Fdata, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
        "val_Fiter = torch.utils.data.DataLoader(val_Fdata, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
        "\n",
        "#iris dataset불러오기 \n",
        "train_Idata = datasets.ImageFolder(DATASET_PATH + '/train/iris', transform=transforms)\n",
        "val_Idata = datasets.ImageFolder(DATASET_PATH + '/validate/iris', transform=transforms)\n",
        "\n",
        "train_Iiter = torch.utils.data.DataLoader(train_Idata, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n",
        "val_Iiter = torch.utils.data.DataLoader(val_Idata, batch_size=BATCH_SIZE, shuffle=True, num_workers=1)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbLfWWVpEdQA"
      },
      "source": [
        "# face 모델\n",
        "face_model = models.resnet18(pretrained=True)\n",
        "num_ftrs = face_model.fc.in_features\n",
        "face_model.fc = nn.Linear(num_ftrs, 64) \n",
        "\n",
        "face_model = face_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(face_model.parameters(), lr=0.001)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "c5cfd094ed634cbf839d05275b174248",
            "953d4da48ba842e6aa76a7af89c22919",
            "a03b7da19ddd4e51a7f9bdd5fe0474fd",
            "e2105ac4e3e542ad83de99b92e4142d1",
            "e35b259cdd034f89b2496c70e307ee49",
            "2333242d83c445ba9b74ff4d6cec3449",
            "646d41ea185e400090f2ba858854944d",
            "0147ac3cac104166bd2b7133d1aee70d"
          ]
        },
        "id": "XbKfUN7E5g8i",
        "outputId": "bf10a957-7ad9-4aa7-cbb5-1640e4ae78e5"
      },
      "source": [
        "# iris 모델\n",
        "iris_model = models.resnet18(pretrained=True)\n",
        "num_ftrs = iris_model.fc.in_features\n",
        "iris_model.fc = nn.Linear(num_ftrs, 64) \n",
        "\n",
        "iris_model = iris_model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(iris_model.parameters(), lr=0.01)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5cfd094ed634cbf839d05275b174248",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ66HbpjEtRe"
      },
      "source": [
        "def test_eval(model, data_iter, batch_size):\n",
        "    with torch.no_grad():\n",
        "        test_loss = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        model.eval()\n",
        "        for batch_img, batch_lab in data_iter:\n",
        "            X = batch_img.to(device)\n",
        "            Y = batch_lab.to(device)\n",
        "            y_pred = model(X)\n",
        "            _, predicted = torch.max(y_pred.data, 1)\n",
        "            correct += (predicted == Y).sum().item()\n",
        "            total += batch_img.size(0)\n",
        "        val_acc = (100 * correct / total)\n",
        "        model.train()\n",
        "    return val_acc"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7-z7gmHjhTW"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yh9gHiieji8H",
        "outputId": "e555820c-e12a-4495-8288-9e2e2972338f"
      },
      "source": [
        "# 홍채 학습\n",
        "print_every = 1\n",
        "print(\"Start training ! (Iris)\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS):\n",
        "    loss_val_sum = 0\n",
        "    for batch_img, batch_lab in train_Iiter:\n",
        "\n",
        "        X = batch_img.to(device)\n",
        "        Y = batch_lab.to(device)\n",
        "        \n",
        "        y_pred = iris_model.forward(X)\n",
        "        loss = criterion(y_pred, Y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_val_sum += loss\n",
        "        \n",
        "    if ((epoch%print_every)==0) or (epoch==(EPOCHS-1)):\n",
        "        loss_val_avg = loss_val_sum / len(train_Iiter)\n",
        "        accr_val = test_eval(iris_model, val_Iiter, BATCH_SIZE)\n",
        "        print(f\"epoch:[{epoch+1}/{EPOCHS}] cost:[{loss_val_avg:.3f}] test_accuracy:[{accr_val:.3f}]\")\n",
        "\n",
        "print(\"Training Done !\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training ! (Iris)\n",
            "epoch:[1/100] cost:[4.560] test_accuracy:[2.344]\n",
            "epoch:[2/100] cost:[3.979] test_accuracy:[3.906]\n",
            "epoch:[3/100] cost:[3.709] test_accuracy:[6.250]\n",
            "epoch:[4/100] cost:[3.451] test_accuracy:[10.156]\n",
            "epoch:[5/100] cost:[3.166] test_accuracy:[18.750]\n",
            "epoch:[6/100] cost:[2.664] test_accuracy:[13.281]\n",
            "epoch:[7/100] cost:[2.334] test_accuracy:[22.656]\n",
            "epoch:[8/100] cost:[1.866] test_accuracy:[29.688]\n",
            "epoch:[9/100] cost:[1.544] test_accuracy:[28.125]\n",
            "epoch:[10/100] cost:[1.488] test_accuracy:[41.406]\n",
            "epoch:[11/100] cost:[1.000] test_accuracy:[28.125]\n",
            "epoch:[12/100] cost:[0.739] test_accuracy:[36.719]\n",
            "epoch:[13/100] cost:[0.615] test_accuracy:[57.031]\n",
            "epoch:[14/100] cost:[0.421] test_accuracy:[90.625]\n",
            "epoch:[15/100] cost:[0.354] test_accuracy:[32.812]\n",
            "epoch:[16/100] cost:[0.394] test_accuracy:[89.844]\n",
            "epoch:[17/100] cost:[0.213] test_accuracy:[47.656]\n",
            "epoch:[18/100] cost:[0.157] test_accuracy:[77.344]\n",
            "epoch:[19/100] cost:[0.157] test_accuracy:[92.188]\n",
            "epoch:[20/100] cost:[0.126] test_accuracy:[80.469]\n",
            "epoch:[21/100] cost:[0.087] test_accuracy:[97.656]\n",
            "epoch:[22/100] cost:[0.072] test_accuracy:[66.406]\n",
            "epoch:[23/100] cost:[0.140] test_accuracy:[88.281]\n",
            "epoch:[24/100] cost:[0.121] test_accuracy:[83.594]\n",
            "epoch:[25/100] cost:[0.113] test_accuracy:[62.500]\n",
            "epoch:[26/100] cost:[0.081] test_accuracy:[96.875]\n",
            "epoch:[27/100] cost:[0.090] test_accuracy:[92.188]\n",
            "epoch:[28/100] cost:[0.036] test_accuracy:[97.656]\n",
            "epoch:[29/100] cost:[0.043] test_accuracy:[99.219]\n",
            "epoch:[30/100] cost:[0.026] test_accuracy:[95.312]\n",
            "epoch:[31/100] cost:[0.019] test_accuracy:[97.656]\n",
            "epoch:[32/100] cost:[0.012] test_accuracy:[100.000]\n",
            "epoch:[33/100] cost:[0.009] test_accuracy:[98.438]\n",
            "epoch:[34/100] cost:[0.006] test_accuracy:[100.000]\n",
            "epoch:[35/100] cost:[0.002] test_accuracy:[100.000]\n",
            "epoch:[36/100] cost:[0.001] test_accuracy:[100.000]\n",
            "epoch:[37/100] cost:[0.001] test_accuracy:[100.000]\n",
            "epoch:[38/100] cost:[0.002] test_accuracy:[99.219]\n",
            "epoch:[39/100] cost:[0.004] test_accuracy:[99.219]\n",
            "epoch:[40/100] cost:[0.002] test_accuracy:[100.000]\n",
            "epoch:[41/100] cost:[0.001] test_accuracy:[100.000]\n",
            "epoch:[42/100] cost:[0.001] test_accuracy:[100.000]\n",
            "epoch:[43/100] cost:[0.002] test_accuracy:[100.000]\n",
            "epoch:[44/100] cost:[0.002] test_accuracy:[100.000]\n",
            "epoch:[45/100] cost:[0.016] test_accuracy:[91.406]\n",
            "epoch:[46/100] cost:[0.057] test_accuracy:[82.031]\n",
            "epoch:[47/100] cost:[0.143] test_accuracy:[74.219]\n",
            "epoch:[48/100] cost:[0.528] test_accuracy:[27.344]\n",
            "epoch:[49/100] cost:[0.420] test_accuracy:[51.562]\n",
            "epoch:[50/100] cost:[0.211] test_accuracy:[89.844]\n",
            "epoch:[51/100] cost:[0.111] test_accuracy:[80.469]\n",
            "epoch:[52/100] cost:[0.087] test_accuracy:[89.844]\n",
            "epoch:[53/100] cost:[0.033] test_accuracy:[99.219]\n",
            "epoch:[54/100] cost:[0.016] test_accuracy:[98.438]\n",
            "epoch:[55/100] cost:[0.034] test_accuracy:[96.875]\n",
            "epoch:[56/100] cost:[0.025] test_accuracy:[96.875]\n",
            "epoch:[57/100] cost:[0.029] test_accuracy:[95.312]\n",
            "epoch:[58/100] cost:[0.061] test_accuracy:[77.344]\n",
            "epoch:[59/100] cost:[0.059] test_accuracy:[93.750]\n",
            "epoch:[60/100] cost:[0.019] test_accuracy:[100.000]\n",
            "epoch:[61/100] cost:[0.008] test_accuracy:[96.875]\n",
            "epoch:[62/100] cost:[0.003] test_accuracy:[99.219]\n",
            "epoch:[63/100] cost:[0.001] test_accuracy:[99.219]\n",
            "epoch:[64/100] cost:[0.001] test_accuracy:[99.219]\n",
            "epoch:[65/100] cost:[0.001] test_accuracy:[99.219]\n",
            "epoch:[66/100] cost:[0.001] test_accuracy:[99.219]\n",
            "epoch:[67/100] cost:[0.001] test_accuracy:[99.219]\n",
            "epoch:[68/100] cost:[0.002] test_accuracy:[100.000]\n",
            "epoch:[69/100] cost:[0.003] test_accuracy:[99.219]\n",
            "epoch:[70/100] cost:[0.001] test_accuracy:[100.000]\n",
            "epoch:[71/100] cost:[0.001] test_accuracy:[100.000]\n",
            "epoch:[72/100] cost:[0.001] test_accuracy:[100.000]\n",
            "epoch:[73/100] cost:[0.001] test_accuracy:[100.000]\n",
            "epoch:[74/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[75/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[76/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[77/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[78/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[79/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[80/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[81/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[82/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[83/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[84/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[85/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[86/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[87/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[88/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[89/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[90/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[91/100] cost:[0.001] test_accuracy:[100.000]\n",
            "epoch:[92/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[93/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[94/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[95/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[96/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[97/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[98/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[99/100] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[100/100] cost:[0.000] test_accuracy:[100.000]\n",
            "Training Done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2r4EZsr6OO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0ca707-20eb-4879-aff2-3f2f5ac09108"
      },
      "source": [
        "# 얼굴 학습\n",
        "\n",
        "\n",
        "print_every = 1\n",
        "print(\"Start training ! (Face)\")\n",
        "\n",
        "EPOCHS_F = 30\n",
        "# Training loop\n",
        "for epoch in range(EPOCHS_F):\n",
        "    loss_val_sum = 0\n",
        "    for batch_img, batch_lab in train_Fiter:\n",
        "\n",
        "        X = batch_img.to(device)\n",
        "        Y = batch_lab.to(device)\n",
        "        \n",
        "        y_pred = face_model.forward(X)\n",
        "        loss = criterion(y_pred, Y)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        loss_val_sum += loss\n",
        "        \n",
        "    if ((epoch%print_every)==0) or (epoch==(EPOCHS_F-1)):\n",
        "        loss_val_avg = loss_val_sum / len(train_Fiter)\n",
        "        accr_val = test_eval(face_model, val_Fiter, BATCH_SIZE)\n",
        "        print(f\"epoch:[{epoch+1}/{EPOCHS_F}] cost:[{loss_val_avg:.3f}] test_accuracy:[{accr_val:.3f}]\")\n",
        "print(\"Training Done !\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start training ! (Face)\n",
            "epoch:[1/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[2/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[3/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[4/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[5/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[6/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[7/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[8/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[9/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[10/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[11/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[12/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[13/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[14/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[15/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[16/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[17/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[18/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[19/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[20/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[21/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[22/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[23/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[24/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[25/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[26/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[27/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[28/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[29/30] cost:[0.000] test_accuracy:[100.000]\n",
            "epoch:[30/30] cost:[0.000] test_accuracy:[100.000]\n",
            "Training Done !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jin6N2lZ6Ypp"
      },
      "source": [
        "#Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1veG0ZF6lcY"
      },
      "source": [
        "###iris validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANF8zUaXkZ9L"
      },
      "source": [
        "val_Iiter = torch.utils.data.DataLoader(val_Idata, batch_size=128, shuffle=True, num_workers=1)\n",
        "data_Iiter = iter(val_Iiter)\n",
        "images, labels = next(data_Iiter)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbyEcxNIkhbS"
      },
      "source": [
        "inputs, classes = next(iter(train_Iiter))\n",
        "out = torchvision.utils.make_grid(inputs)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXTWyv84knfu",
        "outputId": "91fa367e-b1e9-48ab-be7e-8ba03d7ed22d"
      },
      "source": [
        "n_sample = 128\n",
        "test_x = images[:n_sample]\n",
        "test_y = labels[:n_sample]\n",
        "\n",
        "with torch.no_grad():\n",
        "    iris_model.eval()\n",
        "    y_pred = iris_model.forward(test_x.type(torch.float).to(device))\n",
        "    iris_model.train()\n",
        "    \n",
        "y_pred = y_pred.argmax(axis=1)\n",
        "\n",
        "test_accuracy = sum([1 for i, j in zip(y_pred, test_y) if i == j]) / n_sample\n",
        "print(f\"test_accuracy : {test_accuracy:.3f}\")\n",
        "\n",
        "for idx in range(n_sample):\n",
        "    print(\"Predict:\",str(y_pred[idx]+1)[7:10], \"Label:\",str(test_y[idx]+1)[7:10].replace(')', ''))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_accuracy : 1.000\n",
            "Predict: 50, Label: 50\n",
            "Predict: 13, Label: 13\n",
            "Predict: 1,  Label: 1\n",
            "Predict: 47, Label: 47\n",
            "Predict: 46, Label: 46\n",
            "Predict: 26, Label: 26\n",
            "Predict: 2,  Label: 2\n",
            "Predict: 34, Label: 34\n",
            "Predict: 64, Label: 64\n",
            "Predict: 51, Label: 51\n",
            "Predict: 48, Label: 48\n",
            "Predict: 22, Label: 22\n",
            "Predict: 38, Label: 38\n",
            "Predict: 3,  Label: 3\n",
            "Predict: 18, Label: 18\n",
            "Predict: 3,  Label: 3\n",
            "Predict: 15, Label: 15\n",
            "Predict: 6,  Label: 6\n",
            "Predict: 44, Label: 44\n",
            "Predict: 16, Label: 16\n",
            "Predict: 34, Label: 34\n",
            "Predict: 25, Label: 25\n",
            "Predict: 41, Label: 41\n",
            "Predict: 28, Label: 28\n",
            "Predict: 61, Label: 61\n",
            "Predict: 17, Label: 17\n",
            "Predict: 28, Label: 28\n",
            "Predict: 20, Label: 20\n",
            "Predict: 56, Label: 56\n",
            "Predict: 56, Label: 56\n",
            "Predict: 26, Label: 26\n",
            "Predict: 50, Label: 50\n",
            "Predict: 18, Label: 18\n",
            "Predict: 44, Label: 44\n",
            "Predict: 47, Label: 47\n",
            "Predict: 40, Label: 40\n",
            "Predict: 43, Label: 43\n",
            "Predict: 21, Label: 21\n",
            "Predict: 10, Label: 10\n",
            "Predict: 46, Label: 46\n",
            "Predict: 4,  Label: 4\n",
            "Predict: 54, Label: 54\n",
            "Predict: 20, Label: 20\n",
            "Predict: 31, Label: 31\n",
            "Predict: 57, Label: 57\n",
            "Predict: 35, Label: 35\n",
            "Predict: 9,  Label: 9\n",
            "Predict: 33, Label: 33\n",
            "Predict: 23, Label: 23\n",
            "Predict: 15, Label: 15\n",
            "Predict: 62, Label: 62\n",
            "Predict: 5,  Label: 5\n",
            "Predict: 9,  Label: 9\n",
            "Predict: 30, Label: 30\n",
            "Predict: 19, Label: 19\n",
            "Predict: 38, Label: 38\n",
            "Predict: 62, Label: 62\n",
            "Predict: 31, Label: 31\n",
            "Predict: 42, Label: 42\n",
            "Predict: 5,  Label: 5\n",
            "Predict: 59, Label: 59\n",
            "Predict: 49, Label: 49\n",
            "Predict: 8,  Label: 8\n",
            "Predict: 1,  Label: 1\n",
            "Predict: 52, Label: 52\n",
            "Predict: 11, Label: 11\n",
            "Predict: 61, Label: 61\n",
            "Predict: 12, Label: 12\n",
            "Predict: 24, Label: 24\n",
            "Predict: 49, Label: 49\n",
            "Predict: 30, Label: 30\n",
            "Predict: 55, Label: 55\n",
            "Predict: 35, Label: 35\n",
            "Predict: 2,  Label: 2\n",
            "Predict: 52, Label: 52\n",
            "Predict: 54, Label: 54\n",
            "Predict: 39, Label: 39\n",
            "Predict: 43, Label: 43\n",
            "Predict: 45, Label: 45\n",
            "Predict: 58, Label: 58\n",
            "Predict: 42, Label: 42\n",
            "Predict: 37, Label: 37\n",
            "Predict: 51, Label: 51\n",
            "Predict: 27, Label: 27\n",
            "Predict: 32, Label: 32\n",
            "Predict: 64, Label: 64\n",
            "Predict: 29, Label: 29\n",
            "Predict: 55, Label: 55\n",
            "Predict: 25, Label: 25\n",
            "Predict: 36, Label: 36\n",
            "Predict: 32, Label: 32\n",
            "Predict: 19, Label: 19\n",
            "Predict: 58, Label: 58\n",
            "Predict: 63, Label: 63\n",
            "Predict: 36, Label: 36\n",
            "Predict: 14, Label: 14\n",
            "Predict: 6,  Label: 6\n",
            "Predict: 59, Label: 59\n",
            "Predict: 24, Label: 24\n",
            "Predict: 22, Label: 22\n",
            "Predict: 53, Label: 53\n",
            "Predict: 37, Label: 37\n",
            "Predict: 53, Label: 53\n",
            "Predict: 60, Label: 60\n",
            "Predict: 13, Label: 13\n",
            "Predict: 8,  Label: 8\n",
            "Predict: 4,  Label: 4\n",
            "Predict: 21, Label: 21\n",
            "Predict: 16, Label: 16\n",
            "Predict: 17, Label: 17\n",
            "Predict: 45, Label: 45\n",
            "Predict: 57, Label: 57\n",
            "Predict: 7,  Label: 7\n",
            "Predict: 40, Label: 40\n",
            "Predict: 48, Label: 48\n",
            "Predict: 39, Label: 39\n",
            "Predict: 10, Label: 10\n",
            "Predict: 60, Label: 60\n",
            "Predict: 33, Label: 33\n",
            "Predict: 7,  Label: 7\n",
            "Predict: 11, Label: 11\n",
            "Predict: 41, Label: 41\n",
            "Predict: 14, Label: 14\n",
            "Predict: 29, Label: 29\n",
            "Predict: 23, Label: 23\n",
            "Predict: 63, Label: 63\n",
            "Predict: 27, Label: 27\n",
            "Predict: 12, Label: 12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ba4lPY56vcu"
      },
      "source": [
        "###face validation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEJ80ZZW6u7k"
      },
      "source": [
        "val_Fiter = torch.utils.data.DataLoader(val_Fdata, batch_size=128, shuffle=True, num_workers=1)\n",
        "data_Fiter = iter(val_Fiter)\n",
        "images, labels = next(data_Fiter)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aaBYx9b6y7N"
      },
      "source": [
        "inputs, classes = next(iter(train_Fiter))\n",
        "out = torchvision.utils.make_grid(inputs)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IeQLzrC65T-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8b2814e-8b0d-48bd-f007-13ec68560896"
      },
      "source": [
        "n_sample = 128\n",
        "test_x = images[:n_sample]\n",
        "test_y = labels[:n_sample]\n",
        "\n",
        "with torch.no_grad():\n",
        "    face_model.eval()\n",
        "    y_pred = face_model.forward(test_x.type(torch.float).to(device))\n",
        "    face_model.train()\n",
        "    \n",
        "y_pred = y_pred.argmax(axis=1)\n",
        "\n",
        "test_accuracy = sum([1 for i, j in zip(y_pred, test_y) if i == j]) / n_sample\n",
        "print(f\"test_accuracy : {test_accuracy:.3f}\")\n",
        "\n",
        "for idx in range(n_sample):\n",
        "    print(\"Predict:\",str(y_pred[idx]+1)[7:10].replace(')', ''), \"Label:\",str(test_y[idx]+1)[7:10].replace(')', ''))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_accuracy : 1.000\n",
            "Predict: 55, Label: 55\n",
            "Predict: 5,  Label: 5\n",
            "Predict: 59, Label: 59\n",
            "Predict: 24, Label: 24\n",
            "Predict: 43, Label: 43\n",
            "Predict: 32, Label: 32\n",
            "Predict: 6,  Label: 6\n",
            "Predict: 50, Label: 50\n",
            "Predict: 39, Label: 39\n",
            "Predict: 5,  Label: 5\n",
            "Predict: 24, Label: 24\n",
            "Predict: 60, Label: 60\n",
            "Predict: 47, Label: 47\n",
            "Predict: 46, Label: 46\n",
            "Predict: 33, Label: 33\n",
            "Predict: 54, Label: 54\n",
            "Predict: 42, Label: 42\n",
            "Predict: 27, Label: 27\n",
            "Predict: 34, Label: 34\n",
            "Predict: 22, Label: 22\n",
            "Predict: 22, Label: 22\n",
            "Predict: 4,  Label: 4\n",
            "Predict: 28, Label: 28\n",
            "Predict: 35, Label: 35\n",
            "Predict: 36, Label: 36\n",
            "Predict: 7,  Label: 7\n",
            "Predict: 44, Label: 44\n",
            "Predict: 7,  Label: 7\n",
            "Predict: 20, Label: 20\n",
            "Predict: 2,  Label: 2\n",
            "Predict: 3,  Label: 3\n",
            "Predict: 11, Label: 11\n",
            "Predict: 21, Label: 21\n",
            "Predict: 23, Label: 23\n",
            "Predict: 16, Label: 16\n",
            "Predict: 37, Label: 37\n",
            "Predict: 38, Label: 38\n",
            "Predict: 31, Label: 31\n",
            "Predict: 58, Label: 58\n",
            "Predict: 25, Label: 25\n",
            "Predict: 62, Label: 62\n",
            "Predict: 31, Label: 31\n",
            "Predict: 9,  Label: 9\n",
            "Predict: 44, Label: 44\n",
            "Predict: 9,  Label: 9\n",
            "Predict: 14, Label: 14\n",
            "Predict: 39, Label: 39\n",
            "Predict: 41, Label: 41\n",
            "Predict: 48, Label: 48\n",
            "Predict: 47, Label: 47\n",
            "Predict: 2,  Label: 2\n",
            "Predict: 17, Label: 17\n",
            "Predict: 4,  Label: 4\n",
            "Predict: 26, Label: 26\n",
            "Predict: 1,  Label: 1\n",
            "Predict: 15, Label: 15\n",
            "Predict: 25, Label: 25\n",
            "Predict: 29, Label: 29\n",
            "Predict: 49, Label: 49\n",
            "Predict: 34, Label: 34\n",
            "Predict: 27, Label: 27\n",
            "Predict: 53, Label: 53\n",
            "Predict: 30, Label: 30\n",
            "Predict: 60, Label: 60\n",
            "Predict: 41, Label: 41\n",
            "Predict: 64, Label: 64\n",
            "Predict: 53, Label: 53\n",
            "Predict: 8,  Label: 8\n",
            "Predict: 61, Label: 61\n",
            "Predict: 30, Label: 30\n",
            "Predict: 6,  Label: 6\n",
            "Predict: 18, Label: 18\n",
            "Predict: 54, Label: 54\n",
            "Predict: 40, Label: 40\n",
            "Predict: 56, Label: 56\n",
            "Predict: 20, Label: 20\n",
            "Predict: 40, Label: 40\n",
            "Predict: 51, Label: 51\n",
            "Predict: 13, Label: 13\n",
            "Predict: 21, Label: 21\n",
            "Predict: 62, Label: 62\n",
            "Predict: 13, Label: 13\n",
            "Predict: 1,  Label: 1\n",
            "Predict: 14, Label: 14\n",
            "Predict: 42, Label: 42\n",
            "Predict: 46, Label: 46\n",
            "Predict: 19, Label: 19\n",
            "Predict: 33, Label: 33\n",
            "Predict: 8,  Label: 8\n",
            "Predict: 18, Label: 18\n",
            "Predict: 45, Label: 45\n",
            "Predict: 10, Label: 10\n",
            "Predict: 38, Label: 38\n",
            "Predict: 19, Label: 19\n",
            "Predict: 64, Label: 64\n",
            "Predict: 10, Label: 10\n",
            "Predict: 17, Label: 17\n",
            "Predict: 45, Label: 45\n",
            "Predict: 61, Label: 61\n",
            "Predict: 32, Label: 32\n",
            "Predict: 48, Label: 48\n",
            "Predict: 63, Label: 63\n",
            "Predict: 11, Label: 11\n",
            "Predict: 51, Label: 51\n",
            "Predict: 3,  Label: 3\n",
            "Predict: 56, Label: 56\n",
            "Predict: 55, Label: 55\n",
            "Predict: 57, Label: 57\n",
            "Predict: 35, Label: 35\n",
            "Predict: 36, Label: 36\n",
            "Predict: 28, Label: 28\n",
            "Predict: 49, Label: 49\n",
            "Predict: 15, Label: 15\n",
            "Predict: 59, Label: 59\n",
            "Predict: 52, Label: 52\n",
            "Predict: 12, Label: 12\n",
            "Predict: 16, Label: 16\n",
            "Predict: 23, Label: 23\n",
            "Predict: 12, Label: 12\n",
            "Predict: 63, Label: 63\n",
            "Predict: 52, Label: 52\n",
            "Predict: 43, Label: 43\n",
            "Predict: 57, Label: 57\n",
            "Predict: 50, Label: 50\n",
            "Predict: 26, Label: 26\n",
            "Predict: 58, Label: 58\n",
            "Predict: 29, Label: 29\n",
            "Predict: 37, Label: 37\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGDqERQOnv1O"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnznSArE7EXR"
      },
      "source": [
        "###iris test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQNzANrK4lzd"
      },
      "source": [
        "test_data = datasets.ImageFolder(DATASET_PATH + '/test/iris', transform=transforms)\n",
        "test_iter = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False, num_workers=1)\n",
        "\n",
        "class_name = test_data.classes\n",
        "class_len = len(class_name)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH0gpHst4wn-",
        "outputId": "e9984d30-3c94-4585-9b7f-2675defeee00"
      },
      "source": [
        "data_iter = iter(test_iter)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# 라벨링할 class는 train_iter에서\n",
        "inputs, classes = next(iter(train_Iiter))\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "n_sample = 64\n",
        "test_x = images[:n_sample]\n",
        "test_y = labels[:n_sample]\n",
        "\n",
        "with torch.no_grad():\n",
        "    iris_model.eval()\n",
        "    y_pred = iris_model.forward(test_x.type(torch.float).to(device))\n",
        "    iris_model.train()\n",
        "    \n",
        "y_pred = y_pred.argmax(axis=1)\n",
        "\n",
        "result_iris = []\n",
        "\n",
        "for idx in range(n_sample):\n",
        "    print( \"File name:\",str(class_name[test_y[idx]]),\"Predict:\",str(y_pred[idx])[7:10].replace(')',''))\n",
        "    result_iris.append({'Iris image':str(class_name[test_y[idx]]), \n",
        "                        'Iris answer':str(y_pred[idx])[7:10].replace(')','')})\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa4277557a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa4277557a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa4277557a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa4277557a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "File name: 0 Predict: 27,\n",
            "File name: 1 Predict: 60,\n",
            "File name: 10 Predict: 36,\n",
            "File name: 11 Predict: 2, \n",
            "File name: 12 Predict: 46,\n",
            "File name: 13 Predict: 1, \n",
            "File name: 14 Predict: 10,\n",
            "File name: 15 Predict: 61,\n",
            "File name: 16 Predict: 49,\n",
            "File name: 17 Predict: 59,\n",
            "File name: 18 Predict: 11,\n",
            "File name: 19 Predict: 13,\n",
            "File name: 2 Predict: 22,\n",
            "File name: 20 Predict: 37,\n",
            "File name: 21 Predict: 30,\n",
            "File name: 22 Predict: 25,\n",
            "File name: 23 Predict: 0, \n",
            "File name: 24 Predict: 11,\n",
            "File name: 25 Predict: 5, \n",
            "File name: 26 Predict: 9, \n",
            "File name: 27 Predict: 20,\n",
            "File name: 28 Predict: 48,\n",
            "File name: 29 Predict: 47,\n",
            "File name: 3 Predict: 57,\n",
            "File name: 30 Predict: 42,\n",
            "File name: 31 Predict: 24,\n",
            "File name: 32 Predict: 3, \n",
            "File name: 33 Predict: 6, \n",
            "File name: 34 Predict: 55,\n",
            "File name: 35 Predict: 63,\n",
            "File name: 36 Predict: 21,\n",
            "File name: 37 Predict: 16,\n",
            "File name: 38 Predict: 34,\n",
            "File name: 39 Predict: 11,\n",
            "File name: 4 Predict: 54,\n",
            "File name: 40 Predict: 35,\n",
            "File name: 41 Predict: 43,\n",
            "File name: 42 Predict: 19,\n",
            "File name: 43 Predict: 62,\n",
            "File name: 44 Predict: 19,\n",
            "File name: 45 Predict: 52,\n",
            "File name: 46 Predict: 33,\n",
            "File name: 47 Predict: 15,\n",
            "File name: 48 Predict: 1, \n",
            "File name: 49 Predict: 29,\n",
            "File name: 5 Predict: 45,\n",
            "File name: 50 Predict: 17,\n",
            "File name: 51 Predict: 20,\n",
            "File name: 52 Predict: 53,\n",
            "File name: 53 Predict: 28,\n",
            "File name: 54 Predict: 18,\n",
            "File name: 55 Predict: 14,\n",
            "File name: 56 Predict: 43,\n",
            "File name: 57 Predict: 60,\n",
            "File name: 58 Predict: 4, \n",
            "File name: 59 Predict: 50,\n",
            "File name: 6 Predict: 5, \n",
            "File name: 60 Predict: 38,\n",
            "File name: 61 Predict: 23,\n",
            "File name: 62 Predict: 51,\n",
            "File name: 63 Predict: 44,\n",
            "File name: 7 Predict: 1, \n",
            "File name: 8 Predict: 8, \n",
            "File name: 9 Predict: 7, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfv-v5XT5IhD"
      },
      "source": [
        "#Test - Face"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4r9R5JE5TN3"
      },
      "source": [
        "test_data = datasets.ImageFolder(DATASET_PATH + '/test/face', transform=transforms)\n",
        "test_iter = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=False, num_workers=1)\n",
        "\n",
        "class_name = test_data.classes\n",
        "class_len = len(class_name)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFQmdsOF5Wvq",
        "outputId": "7722119d-f8df-430b-e664-c0d22d14bb1c"
      },
      "source": [
        "data_iter = iter(test_iter)\n",
        "images, labels = next(data_iter)\n",
        "\n",
        "# 라벨링할 class는 train_iter에서\n",
        "inputs, classes = next(iter(train_Fiter))\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "n_sample = 64\n",
        "test_x = images[:n_sample]\n",
        "test_y = labels[:n_sample]\n",
        "\n",
        "with torch.no_grad():\n",
        "    face_model.eval()\n",
        "    y_pred = face_model.forward(test_x.type(torch.float).to(device))\n",
        "    face_model.train()\n",
        "    \n",
        "y_pred = y_pred.argmax(axis=1)\n",
        "\n",
        "result_face = []\n",
        "\n",
        "for idx in range(n_sample):\n",
        "    print( \"File name:\",str(class_name[test_y[idx]]),\"Predict:\",str(y_pred[idx])[7:10])\n",
        "    result_face.append({'Face image':str(class_name[test_y[idx]]), \n",
        "                        'Face answer':str(y_pred[idx])[7:10].replace(')','')})\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa4277557a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa4277557a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa4277557a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7fa4277557a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1324, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1316, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 151, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "AssertionError: can only test a child process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "File name: 0 Predict: 27,\n",
            "File name: 1 Predict: 31,\n",
            "File name: 10 Predict: 36,\n",
            "File name: 11 Predict: 53,\n",
            "File name: 12 Predict: 46,\n",
            "File name: 13 Predict: 50,\n",
            "File name: 14 Predict: 10,\n",
            "File name: 15 Predict: 61,\n",
            "File name: 16 Predict: 49,\n",
            "File name: 17 Predict: 59,\n",
            "File name: 18 Predict: 56,\n",
            "File name: 19 Predict: 13,\n",
            "File name: 2 Predict: 22,\n",
            "File name: 20 Predict: 37,\n",
            "File name: 21 Predict: 30,\n",
            "File name: 22 Predict: 25,\n",
            "File name: 23 Predict: 19,\n",
            "File name: 24 Predict: 41,\n",
            "File name: 25 Predict: 58,\n",
            "File name: 26 Predict: 9, \n",
            "File name: 27 Predict: 20,\n",
            "File name: 28 Predict: 48,\n",
            "File name: 29 Predict: 47,\n",
            "File name: 3 Predict: 57,\n",
            "File name: 30 Predict: 42,\n",
            "File name: 31 Predict: 24,\n",
            "File name: 32 Predict: 3, \n",
            "File name: 33 Predict: 6, \n",
            "File name: 34 Predict: 55,\n",
            "File name: 35 Predict: 63,\n",
            "File name: 36 Predict: 21,\n",
            "File name: 37 Predict: 16,\n",
            "File name: 38 Predict: 34,\n",
            "File name: 39 Predict: 11,\n",
            "File name: 4 Predict: 54,\n",
            "File name: 40 Predict: 35,\n",
            "File name: 41 Predict: 32,\n",
            "File name: 42 Predict: 40,\n",
            "File name: 43 Predict: 62,\n",
            "File name: 44 Predict: 19,\n",
            "File name: 45 Predict: 52,\n",
            "File name: 46 Predict: 33,\n",
            "File name: 47 Predict: 15,\n",
            "File name: 48 Predict: 13,\n",
            "File name: 49 Predict: 29,\n",
            "File name: 5 Predict: 45,\n",
            "File name: 50 Predict: 17,\n",
            "File name: 51 Predict: 26,\n",
            "File name: 52 Predict: 53,\n",
            "File name: 53 Predict: 28,\n",
            "File name: 54 Predict: 18,\n",
            "File name: 55 Predict: 14,\n",
            "File name: 56 Predict: 43,\n",
            "File name: 57 Predict: 60,\n",
            "File name: 58 Predict: 4, \n",
            "File name: 59 Predict: 39,\n",
            "File name: 6 Predict: 5, \n",
            "File name: 60 Predict: 38,\n",
            "File name: 61 Predict: 23,\n",
            "File name: 62 Predict: 51,\n",
            "File name: 63 Predict: 44,\n",
            "File name: 7 Predict: 12,\n",
            "File name: 8 Predict: 8, \n",
            "File name: 9 Predict: 7, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55bjeLIX286J",
        "outputId": "0b6ca119-bb8c-43ec-9763-6285b726f089"
      },
      "source": [
        "result_face"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Face answer': '27,', 'Face image': '0'},\n",
              " {'Face answer': '31,', 'Face image': '1'},\n",
              " {'Face answer': '36,', 'Face image': '10'},\n",
              " {'Face answer': '53,', 'Face image': '11'},\n",
              " {'Face answer': '46,', 'Face image': '12'},\n",
              " {'Face answer': '50,', 'Face image': '13'},\n",
              " {'Face answer': '10,', 'Face image': '14'},\n",
              " {'Face answer': '61,', 'Face image': '15'},\n",
              " {'Face answer': '49,', 'Face image': '16'},\n",
              " {'Face answer': '59,', 'Face image': '17'},\n",
              " {'Face answer': '56,', 'Face image': '18'},\n",
              " {'Face answer': '13,', 'Face image': '19'},\n",
              " {'Face answer': '22,', 'Face image': '2'},\n",
              " {'Face answer': '37,', 'Face image': '20'},\n",
              " {'Face answer': '30,', 'Face image': '21'},\n",
              " {'Face answer': '25,', 'Face image': '22'},\n",
              " {'Face answer': '19,', 'Face image': '23'},\n",
              " {'Face answer': '41,', 'Face image': '24'},\n",
              " {'Face answer': '58,', 'Face image': '25'},\n",
              " {'Face answer': '9, ', 'Face image': '26'},\n",
              " {'Face answer': '20,', 'Face image': '27'},\n",
              " {'Face answer': '48,', 'Face image': '28'},\n",
              " {'Face answer': '47,', 'Face image': '29'},\n",
              " {'Face answer': '57,', 'Face image': '3'},\n",
              " {'Face answer': '42,', 'Face image': '30'},\n",
              " {'Face answer': '24,', 'Face image': '31'},\n",
              " {'Face answer': '3, ', 'Face image': '32'},\n",
              " {'Face answer': '6, ', 'Face image': '33'},\n",
              " {'Face answer': '55,', 'Face image': '34'},\n",
              " {'Face answer': '63,', 'Face image': '35'},\n",
              " {'Face answer': '21,', 'Face image': '36'},\n",
              " {'Face answer': '16,', 'Face image': '37'},\n",
              " {'Face answer': '34,', 'Face image': '38'},\n",
              " {'Face answer': '11,', 'Face image': '39'},\n",
              " {'Face answer': '54,', 'Face image': '4'},\n",
              " {'Face answer': '35,', 'Face image': '40'},\n",
              " {'Face answer': '32,', 'Face image': '41'},\n",
              " {'Face answer': '40,', 'Face image': '42'},\n",
              " {'Face answer': '62,', 'Face image': '43'},\n",
              " {'Face answer': '19,', 'Face image': '44'},\n",
              " {'Face answer': '52,', 'Face image': '45'},\n",
              " {'Face answer': '33,', 'Face image': '46'},\n",
              " {'Face answer': '15,', 'Face image': '47'},\n",
              " {'Face answer': '13,', 'Face image': '48'},\n",
              " {'Face answer': '29,', 'Face image': '49'},\n",
              " {'Face answer': '45,', 'Face image': '5'},\n",
              " {'Face answer': '17,', 'Face image': '50'},\n",
              " {'Face answer': '26,', 'Face image': '51'},\n",
              " {'Face answer': '53,', 'Face image': '52'},\n",
              " {'Face answer': '28,', 'Face image': '53'},\n",
              " {'Face answer': '18,', 'Face image': '54'},\n",
              " {'Face answer': '14,', 'Face image': '55'},\n",
              " {'Face answer': '43,', 'Face image': '56'},\n",
              " {'Face answer': '60,', 'Face image': '57'},\n",
              " {'Face answer': '4, ', 'Face image': '58'},\n",
              " {'Face answer': '39,', 'Face image': '59'},\n",
              " {'Face answer': '5, ', 'Face image': '6'},\n",
              " {'Face answer': '38,', 'Face image': '60'},\n",
              " {'Face answer': '23,', 'Face image': '61'},\n",
              " {'Face answer': '51,', 'Face image': '62'},\n",
              " {'Face answer': '44,', 'Face image': '63'},\n",
              " {'Face answer': '12,', 'Face image': '7'},\n",
              " {'Face answer': '8, ', 'Face image': '8'},\n",
              " {'Face answer': '7, ', 'Face image': '9'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2-Sm8iD3AGB",
        "outputId": "923dd046-9366-4204-b892-d705bee990a9"
      },
      "source": [
        "result_iris"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'Iris answer': '27,', 'Iris image': '0'},\n",
              " {'Iris answer': '60,', 'Iris image': '1'},\n",
              " {'Iris answer': '36,', 'Iris image': '10'},\n",
              " {'Iris answer': '2, ', 'Iris image': '11'},\n",
              " {'Iris answer': '46,', 'Iris image': '12'},\n",
              " {'Iris answer': '1, ', 'Iris image': '13'},\n",
              " {'Iris answer': '10,', 'Iris image': '14'},\n",
              " {'Iris answer': '61,', 'Iris image': '15'},\n",
              " {'Iris answer': '49,', 'Iris image': '16'},\n",
              " {'Iris answer': '59,', 'Iris image': '17'},\n",
              " {'Iris answer': '11,', 'Iris image': '18'},\n",
              " {'Iris answer': '13,', 'Iris image': '19'},\n",
              " {'Iris answer': '22,', 'Iris image': '2'},\n",
              " {'Iris answer': '37,', 'Iris image': '20'},\n",
              " {'Iris answer': '30,', 'Iris image': '21'},\n",
              " {'Iris answer': '25,', 'Iris image': '22'},\n",
              " {'Iris answer': '0, ', 'Iris image': '23'},\n",
              " {'Iris answer': '11,', 'Iris image': '24'},\n",
              " {'Iris answer': '5, ', 'Iris image': '25'},\n",
              " {'Iris answer': '9, ', 'Iris image': '26'},\n",
              " {'Iris answer': '20,', 'Iris image': '27'},\n",
              " {'Iris answer': '48,', 'Iris image': '28'},\n",
              " {'Iris answer': '47,', 'Iris image': '29'},\n",
              " {'Iris answer': '57,', 'Iris image': '3'},\n",
              " {'Iris answer': '42,', 'Iris image': '30'},\n",
              " {'Iris answer': '24,', 'Iris image': '31'},\n",
              " {'Iris answer': '3, ', 'Iris image': '32'},\n",
              " {'Iris answer': '6, ', 'Iris image': '33'},\n",
              " {'Iris answer': '55,', 'Iris image': '34'},\n",
              " {'Iris answer': '63,', 'Iris image': '35'},\n",
              " {'Iris answer': '21,', 'Iris image': '36'},\n",
              " {'Iris answer': '16,', 'Iris image': '37'},\n",
              " {'Iris answer': '34,', 'Iris image': '38'},\n",
              " {'Iris answer': '11,', 'Iris image': '39'},\n",
              " {'Iris answer': '54,', 'Iris image': '4'},\n",
              " {'Iris answer': '35,', 'Iris image': '40'},\n",
              " {'Iris answer': '43,', 'Iris image': '41'},\n",
              " {'Iris answer': '19,', 'Iris image': '42'},\n",
              " {'Iris answer': '62,', 'Iris image': '43'},\n",
              " {'Iris answer': '19,', 'Iris image': '44'},\n",
              " {'Iris answer': '52,', 'Iris image': '45'},\n",
              " {'Iris answer': '33,', 'Iris image': '46'},\n",
              " {'Iris answer': '15,', 'Iris image': '47'},\n",
              " {'Iris answer': '1, ', 'Iris image': '48'},\n",
              " {'Iris answer': '29,', 'Iris image': '49'},\n",
              " {'Iris answer': '45,', 'Iris image': '5'},\n",
              " {'Iris answer': '17,', 'Iris image': '50'},\n",
              " {'Iris answer': '20,', 'Iris image': '51'},\n",
              " {'Iris answer': '53,', 'Iris image': '52'},\n",
              " {'Iris answer': '28,', 'Iris image': '53'},\n",
              " {'Iris answer': '18,', 'Iris image': '54'},\n",
              " {'Iris answer': '14,', 'Iris image': '55'},\n",
              " {'Iris answer': '43,', 'Iris image': '56'},\n",
              " {'Iris answer': '60,', 'Iris image': '57'},\n",
              " {'Iris answer': '4, ', 'Iris image': '58'},\n",
              " {'Iris answer': '50,', 'Iris image': '59'},\n",
              " {'Iris answer': '5, ', 'Iris image': '6'},\n",
              " {'Iris answer': '38,', 'Iris image': '60'},\n",
              " {'Iris answer': '23,', 'Iris image': '61'},\n",
              " {'Iris answer': '51,', 'Iris image': '62'},\n",
              " {'Iris answer': '44,', 'Iris image': '63'},\n",
              " {'Iris answer': '1, ', 'Iris image': '7'},\n",
              " {'Iris answer': '8, ', 'Iris image': '8'},\n",
              " {'Iris answer': '7, ', 'Iris image': '9'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk6g3Q0XkN5F",
        "outputId": "a161c897-2488-4b32-c85e-13e1536eccc7"
      },
      "source": [
        "result = []\n",
        "\n",
        "for i in range(64):\n",
        "  iris_pred = result_iris[i].get('Iris answer')\n",
        "  face_pred = result_face[i].get('Face answer')\n",
        "  label = result_iris[i].get('Iris image')\n",
        "  print(label)\n",
        "  print(\"iris pred : \",iris_pred, \"  face pred : \",face_pred)\n",
        "  if (iris_pred != face_pred):\n",
        "    result.append({'image':label,'answer':iris_pred})\n",
        "  else :\n",
        "    result.append({'image':label,'answer':iris_pred})"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "iris pred :  27,   face pred :  27,\n",
            "1\n",
            "iris pred :  60,   face pred :  31,\n",
            "10\n",
            "iris pred :  36,   face pred :  36,\n",
            "11\n",
            "iris pred :  2,    face pred :  53,\n",
            "12\n",
            "iris pred :  46,   face pred :  46,\n",
            "13\n",
            "iris pred :  1,    face pred :  50,\n",
            "14\n",
            "iris pred :  10,   face pred :  10,\n",
            "15\n",
            "iris pred :  61,   face pred :  61,\n",
            "16\n",
            "iris pred :  49,   face pred :  49,\n",
            "17\n",
            "iris pred :  59,   face pred :  59,\n",
            "18\n",
            "iris pred :  11,   face pred :  56,\n",
            "19\n",
            "iris pred :  13,   face pred :  13,\n",
            "2\n",
            "iris pred :  22,   face pred :  22,\n",
            "20\n",
            "iris pred :  37,   face pred :  37,\n",
            "21\n",
            "iris pred :  30,   face pred :  30,\n",
            "22\n",
            "iris pred :  25,   face pred :  25,\n",
            "23\n",
            "iris pred :  0,    face pred :  19,\n",
            "24\n",
            "iris pred :  11,   face pred :  41,\n",
            "25\n",
            "iris pred :  5,    face pred :  58,\n",
            "26\n",
            "iris pred :  9,    face pred :  9, \n",
            "27\n",
            "iris pred :  20,   face pred :  20,\n",
            "28\n",
            "iris pred :  48,   face pred :  48,\n",
            "29\n",
            "iris pred :  47,   face pred :  47,\n",
            "3\n",
            "iris pred :  57,   face pred :  57,\n",
            "30\n",
            "iris pred :  42,   face pred :  42,\n",
            "31\n",
            "iris pred :  24,   face pred :  24,\n",
            "32\n",
            "iris pred :  3,    face pred :  3, \n",
            "33\n",
            "iris pred :  6,    face pred :  6, \n",
            "34\n",
            "iris pred :  55,   face pred :  55,\n",
            "35\n",
            "iris pred :  63,   face pred :  63,\n",
            "36\n",
            "iris pred :  21,   face pred :  21,\n",
            "37\n",
            "iris pred :  16,   face pred :  16,\n",
            "38\n",
            "iris pred :  34,   face pred :  34,\n",
            "39\n",
            "iris pred :  11,   face pred :  11,\n",
            "4\n",
            "iris pred :  54,   face pred :  54,\n",
            "40\n",
            "iris pred :  35,   face pred :  35,\n",
            "41\n",
            "iris pred :  43,   face pred :  32,\n",
            "42\n",
            "iris pred :  19,   face pred :  40,\n",
            "43\n",
            "iris pred :  62,   face pred :  62,\n",
            "44\n",
            "iris pred :  19,   face pred :  19,\n",
            "45\n",
            "iris pred :  52,   face pred :  52,\n",
            "46\n",
            "iris pred :  33,   face pred :  33,\n",
            "47\n",
            "iris pred :  15,   face pred :  15,\n",
            "48\n",
            "iris pred :  1,    face pred :  13,\n",
            "49\n",
            "iris pred :  29,   face pred :  29,\n",
            "5\n",
            "iris pred :  45,   face pred :  45,\n",
            "50\n",
            "iris pred :  17,   face pred :  17,\n",
            "51\n",
            "iris pred :  20,   face pred :  26,\n",
            "52\n",
            "iris pred :  53,   face pred :  53,\n",
            "53\n",
            "iris pred :  28,   face pred :  28,\n",
            "54\n",
            "iris pred :  18,   face pred :  18,\n",
            "55\n",
            "iris pred :  14,   face pred :  14,\n",
            "56\n",
            "iris pred :  43,   face pred :  43,\n",
            "57\n",
            "iris pred :  60,   face pred :  60,\n",
            "58\n",
            "iris pred :  4,    face pred :  4, \n",
            "59\n",
            "iris pred :  50,   face pred :  39,\n",
            "6\n",
            "iris pred :  5,    face pred :  5, \n",
            "60\n",
            "iris pred :  38,   face pred :  38,\n",
            "61\n",
            "iris pred :  23,   face pred :  23,\n",
            "62\n",
            "iris pred :  51,   face pred :  51,\n",
            "63\n",
            "iris pred :  44,   face pred :  44,\n",
            "7\n",
            "iris pred :  1,    face pred :  12,\n",
            "8\n",
            "iris pred :  8,    face pred :  8, \n",
            "9\n",
            "iris pred :  7,    face pred :  7, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtWhHlkvnJEg"
      },
      "source": [
        "from openpyxl import Workbook # 값을 자동으로 엑셀에 저장하는 코드\n",
        " \n",
        "wb = Workbook()\n",
        "\n",
        "sheet1 = wb.active\n",
        "\n",
        "sheet1.title = 'sampleSheet'\n",
        "fname = 'result_멀티모달2개_2차.xlsx'\n",
        "\n",
        "n = len(result)\n",
        "\n",
        "for i in range(n):\n",
        "  sheet1.cell(row=i+1, column=1).value = result[i].get('image')\n",
        "  sheet1.cell(row=i+1, column=2).value =  str(result[i].get('answer')).replace(',','')\n",
        "\n",
        "wb.save(filename=fname)"
      ],
      "execution_count": 39,
      "outputs": []
    }
  ]
}